{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages needed\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import scipy.stats\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some variables and objects needed\n",
    "unwanted = nltk.corpus.stopwords.words(\"english\")\n",
    "names = nltk.corpus.names.words()\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the functions\n",
    "# raw counts of words\n",
    "def get_top_words_mean_std(corpus, N=None, drop_unwanted = True, unwanted = nltk.corpus.stopwords.words(\"english\")):\n",
    "    vec = CountVectorizer()\n",
    "    bag_of_words = vec.fit_transform(corpus)\n",
    "    std_words = bag_of_words.toarray().std(axis=0) \n",
    "    mean_words = bag_of_words.toarray().mean(axis=0)\n",
    "    words_mean_std = [tuple(x) for x in zip(vec.get_feature_names(), mean_words, std_words)]\n",
    "    words_mean_std = sorted(words_mean_std, key = lambda x: x[1], reverse=True)\n",
    "    \n",
    "    df = pd.DataFrame(words_mean_std, columns = ['word', 'avg_mentions', 'std_mentions']).sort_values('avg_mentions', ascending = False)\n",
    "    if drop_unwanted == True:\n",
    "        df = df[~df.word.isin(unwanted)]\n",
    "    return df.head(n=N)\n",
    "\n",
    "# binary count - 1 if it appears, 0 otherwise\n",
    "def get_top_words_proportions(corpus, N=None, drop_unwanted = True, unwanted = nltk.corpus.stopwords.words(\"english\")):\n",
    "    vec = CountVectorizer(binary = True)\n",
    "    bag_of_words = vec.fit_transform(corpus)\n",
    "    proportion_words = bag_of_words.toarray().mean(axis=0)\n",
    "    words_proportion = [tuple(x) for x in zip(vec.get_feature_names(), proportion_words)]\n",
    "    words_proportion = sorted(words_proportion, key = lambda x: x[1], reverse=True)\n",
    "    \n",
    "    df = pd.DataFrame(words_proportion, columns = ['word', 'percent_containing']).sort_values('percent_containing', ascending = False)\n",
    "    if drop_unwanted == True:\n",
    "        df = df[~df.word.isin(unwanted)]\n",
    "    return df.head(n=N)\n",
    "\n",
    "\n",
    "def get_top_words_stats_df(df, corpus_col, N=None, drop_unwanted = True, unwanted = nltk.corpus.stopwords.words(\"english\")):\n",
    "    \n",
    "    corpus = df[corpus_col].to_list()\n",
    "    \n",
    "    mean_std_df = get_top_words_mean_std(corpus, None, drop_unwanted, unwanted)\n",
    "    proportion_df = get_top_words_proportions(corpus, None, drop_unwanted, unwanted)\n",
    "    \n",
    "    all_stats_df = mean_std_df.merge(proportion_df, on = 'word', how = 'outer').fillna(0)\n",
    "    \n",
    "    return all_stats_df.head(n=N)\n",
    "\n",
    "def sentiment_of_word(df, corpus_col, word, sensitivity = .75):\n",
    "        print('Word: ', word)\n",
    "        \n",
    "        corpus = df[corpus_col].to_list()\n",
    "        \n",
    "        count = 0\n",
    "        pos_count = 0\n",
    "        neg_count = 0\n",
    "        neut_count = 0\n",
    "        for entry in corpus:\n",
    "            for sentence in nltk.sent_tokenize(entry):\n",
    "                if ' ' + word + ' ' in sentence:\n",
    "                    count += 1\n",
    "                    if sia.polarity_scores(sentence)[\"compound\"] > sensitivity:\n",
    "                        pos_count += 1\n",
    "                    elif sia.polarity_scores(sentence)[\"compound\"] < -1 * sensitivity:\n",
    "                        neg_count += 1\n",
    "                    else:\n",
    "                        neut_count += 1\n",
    "        \n",
    "        print('\\tPercent of mentions that are positive: {0:.3f}'\\\n",
    "             .format(pos_count / count))\n",
    "        \n",
    "        print('\\tPercent of mentions that are negative: {0:.3f}'\\\n",
    "             .format(neg_count / count))\n",
    "\n",
    "\n",
    "def get_common_bigrams(df, corpus_col, N=None):\n",
    "    corpus = df[corpus_col].to_list()\n",
    "    tokenized_review = [nltk.word_tokenize(review) for review in corpus]\n",
    "    cleaned_reviews = [[word for word in review if word not in unwanted\\\n",
    "        if word.isalpha()] for review in tokenized_review]\n",
    "    \n",
    "    bigrams = Counter({})\n",
    "\n",
    "    for review in cleaned_reviews:\n",
    "        bigrams += Counter(nltk.collocations.BigramCollocationFinder.from_words(review).ngram_fd.keys())\n",
    "    \n",
    "    bigram_df = pd.DataFrame(bigrams.most_common(N), columns = ['bigram', 'percent_containing'])\n",
    "    bigram_df['percent_containing'] = bigram_df['percent_containing'] / df[corpus_col].count()\n",
    "   \n",
    "    bigram_df['bigram'] = bigram_df['bigram'].apply(lambda x: x[0] + ' ' + x[1])\n",
    "\n",
    "    return bigram_df.sort_values('percent_containing', ascending = False).head(n=N)\n",
    "\n",
    " \n",
    "def contains_theme(x, theme):\n",
    "    if re.compile('|'.join(theme),re.IGNORECASE).search(x):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def get_theme_frequency(df, corpus_col, theme):\n",
    "    temp = df\n",
    "    temp['contains_theme'] = temp[corpus_col].apply(lambda x: contains_theme(x, theme))\n",
    "    print('Theme words: ', theme)\n",
    "    print('Percent of entries containing theme: {0:.3f}'.format(temp.contains_theme.mean()))\n",
    "    \n",
    "def sentiment_of_theme(df, corpus_col, theme, sensitivity = .75):\n",
    "    print('Theme words: ', theme)\n",
    "    corpus = df[corpus_col].to_list()\n",
    "    count = 0\n",
    "    pos_count = 0\n",
    "    neg_count = 0\n",
    "    neut_count = 0\n",
    "    for entry in corpus:\n",
    "        for sentence in nltk.sent_tokenize(entry):\n",
    "            if re.compile('|'.join(theme),re.IGNORECASE).search(sentence):\n",
    "                    count += 1\n",
    "                    if sia.polarity_scores(sentence)[\"compound\"] > sensitivity:\n",
    "                        pos_count += 1\n",
    "                    elif sia.polarity_scores(sentence)[\"compound\"] < -1 * sensitivity:\n",
    "                        neg_count += 1\n",
    "                    else:\n",
    "                        neut_count += 1\n",
    "        \n",
    "    print('\\tPercent of mentions that are positive: {0:.3f}'\\\n",
    "         .format(pos_count / count))\n",
    "\n",
    "    print('\\tPercent of mentions that are negative: {0:.3f}'\\\n",
    "         .format(neg_count / count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe to practice with\n",
    "positive_review_ids = nltk.corpus.movie_reviews.fileids(categories=[\"pos\"])\n",
    "negative_review_ids = nltk.corpus.movie_reviews.fileids(categories=[\"neg\"])\n",
    "pos_df = pd.DataFrame(positive_review_ids, columns = ['review_id'])\n",
    "pos_df['rating'] = 'pos'\n",
    "neg_df = pd.DataFrame(negative_review_ids, columns = ['review_id'])\n",
    "neg_df['rating'] = 'neg'\n",
    "rating_df = pd.concat([pos_df, neg_df]).reset_index().drop('index', axis = 1)\n",
    "rating_df['review_text'] = rating_df.review_id.apply(lambda x: nltk.corpus.movie_reviews.raw(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pos/cv000_29590.txt</td>\n",
       "      <td>pos</td>\n",
       "      <td>films adapted from comic books have had plenty...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pos/cv001_18431.txt</td>\n",
       "      <td>pos</td>\n",
       "      <td>every now and then a movie comes along from a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos/cv002_15918.txt</td>\n",
       "      <td>pos</td>\n",
       "      <td>you've got mail works alot better than it dese...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pos/cv003_11664.txt</td>\n",
       "      <td>pos</td>\n",
       "      <td>\" jaws \" is a rare film that grabs your atten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pos/cv004_11636.txt</td>\n",
       "      <td>pos</td>\n",
       "      <td>moviemaking is a lot like being the general ma...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             review_id rating  \\\n",
       "0  pos/cv000_29590.txt    pos   \n",
       "1  pos/cv001_18431.txt    pos   \n",
       "2  pos/cv002_15918.txt    pos   \n",
       "3  pos/cv003_11664.txt    pos   \n",
       "4  pos/cv004_11636.txt    pos   \n",
       "\n",
       "                                         review_text  \n",
       "0  films adapted from comic books have had plenty...  \n",
       "1  every now and then a movie comes along from a ...  \n",
       "2  you've got mail works alot better than it dese...  \n",
       "3   \" jaws \" is a rare film that grabs your atten...  \n",
       "4  moviemaking is a lot like being the general ma...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = rating_df\n",
    "df.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which words show up most in all reviews?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>avg_mentions</th>\n",
       "      <th>std_mentions</th>\n",
       "      <th>percent_containing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>film</td>\n",
       "      <td>4.7585</td>\n",
       "      <td>4.475173</td>\n",
       "      <td>0.8765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>one</td>\n",
       "      <td>2.9260</td>\n",
       "      <td>2.398234</td>\n",
       "      <td>0.8880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>movie</td>\n",
       "      <td>2.8855</td>\n",
       "      <td>3.230850</td>\n",
       "      <td>0.7770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>like</td>\n",
       "      <td>1.8450</td>\n",
       "      <td>1.842274</td>\n",
       "      <td>0.7485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>even</td>\n",
       "      <td>1.2825</td>\n",
       "      <td>1.404170</td>\n",
       "      <td>0.6455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>good</td>\n",
       "      <td>1.2055</td>\n",
       "      <td>1.497087</td>\n",
       "      <td>0.5910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>time</td>\n",
       "      <td>1.2055</td>\n",
       "      <td>1.354352</td>\n",
       "      <td>0.6290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>story</td>\n",
       "      <td>1.0845</td>\n",
       "      <td>1.533414</td>\n",
       "      <td>0.5390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>would</td>\n",
       "      <td>1.0545</td>\n",
       "      <td>1.310927</td>\n",
       "      <td>0.5675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>much</td>\n",
       "      <td>1.0245</td>\n",
       "      <td>1.231625</td>\n",
       "      <td>0.5690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>character</td>\n",
       "      <td>1.0100</td>\n",
       "      <td>1.362681</td>\n",
       "      <td>0.5235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>also</td>\n",
       "      <td>0.9835</td>\n",
       "      <td>1.293920</td>\n",
       "      <td>0.5355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>get</td>\n",
       "      <td>0.9745</td>\n",
       "      <td>1.213610</td>\n",
       "      <td>0.5500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>two</td>\n",
       "      <td>0.9555</td>\n",
       "      <td>1.246002</td>\n",
       "      <td>0.5310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>well</td>\n",
       "      <td>0.9530</td>\n",
       "      <td>1.218930</td>\n",
       "      <td>0.5400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         word  avg_mentions  std_mentions  percent_containing\n",
       "0        film        4.7585      4.475173              0.8765\n",
       "1         one        2.9260      2.398234              0.8880\n",
       "2       movie        2.8855      3.230850              0.7770\n",
       "3        like        1.8450      1.842274              0.7485\n",
       "4        even        1.2825      1.404170              0.6455\n",
       "5        good        1.2055      1.497087              0.5910\n",
       "6        time        1.2055      1.354352              0.6290\n",
       "7       story        1.0845      1.533414              0.5390\n",
       "8       would        1.0545      1.310927              0.5675\n",
       "9        much        1.0245      1.231625              0.5690\n",
       "10  character        1.0100      1.362681              0.5235\n",
       "11       also        0.9835      1.293920              0.5355\n",
       "12        get        0.9745      1.213610              0.5500\n",
       "13        two        0.9555      1.246002              0.5310\n",
       "14       well        0.9530      1.218930              0.5400"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_freqs = get_top_words_stats_df(df, 'review_text')\n",
    "term_freqs.head(n=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These seem like generic terms that we might expect in all reviews. Are there differences between the positive and negative reviews?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>avg_mentions</th>\n",
       "      <th>std_mentions</th>\n",
       "      <th>percent_containing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>film</td>\n",
       "      <td>5.230</td>\n",
       "      <td>4.846555</td>\n",
       "      <td>0.897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>one</td>\n",
       "      <td>3.052</td>\n",
       "      <td>2.507448</td>\n",
       "      <td>0.899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>movie</td>\n",
       "      <td>2.525</td>\n",
       "      <td>2.984858</td>\n",
       "      <td>0.739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>like</td>\n",
       "      <td>1.802</td>\n",
       "      <td>1.891242</td>\n",
       "      <td>0.724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>good</td>\n",
       "      <td>1.248</td>\n",
       "      <td>1.540940</td>\n",
       "      <td>0.596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    word  avg_mentions  std_mentions  percent_containing\n",
       "0   film         5.230      4.846555               0.897\n",
       "1    one         3.052      2.507448               0.899\n",
       "2  movie         2.525      2.984858               0.739\n",
       "3   like         1.802      1.891242               0.724\n",
       "4   good         1.248      1.540940               0.596"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_freqs_pos = get_top_words_stats_df(df[df.rating=='pos'], 'review_text')\n",
    "term_freqs_pos.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>avg_mentions</th>\n",
       "      <th>std_mentions</th>\n",
       "      <th>percent_containing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>film</td>\n",
       "      <td>4.287</td>\n",
       "      <td>4.015051</td>\n",
       "      <td>0.856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>movie</td>\n",
       "      <td>3.246</td>\n",
       "      <td>3.421620</td>\n",
       "      <td>0.815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>one</td>\n",
       "      <td>2.800</td>\n",
       "      <td>2.276840</td>\n",
       "      <td>0.877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>like</td>\n",
       "      <td>1.888</td>\n",
       "      <td>1.790937</td>\n",
       "      <td>0.773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>even</td>\n",
       "      <td>1.386</td>\n",
       "      <td>1.447413</td>\n",
       "      <td>0.672</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    word  avg_mentions  std_mentions  percent_containing\n",
       "0   film         4.287      4.015051               0.856\n",
       "1  movie         3.246      3.421620               0.815\n",
       "2    one         2.800      2.276840               0.877\n",
       "3   like         1.888      1.790937               0.773\n",
       "4   even         1.386      1.447413               0.672"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_freqs_neg = get_top_words_stats_df(df[df.rating=='neg'], 'review_text')\n",
    "term_freqs_neg.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like 'film' could be a signal for movies with positive ratings, while 'movie' may be used more in negative reviews. Where do the greatest absolute differences exist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "catch = term_freqs_pos.merge(term_freqs_neg, on = 'word', how = 'outer', suffixes = ['_pos', '_neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>abs_diff</th>\n",
       "      <th>percent_containing_pos</th>\n",
       "      <th>percent_containing_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>bad</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>life</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1367</th>\n",
       "      <td>worst</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>plot</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>also</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>script</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>great</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>best</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.489</td>\n",
       "      <td>0.365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>world</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1313</th>\n",
       "      <td>boring</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>many</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>nothing</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1483</th>\n",
       "      <td>stupid</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>performances</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>perfect</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              word  abs_diff  percent_containing_pos  percent_containing_neg\n",
       "108            bad     0.255                   0.259                   0.514\n",
       "11            life     0.158                   0.492                   0.334\n",
       "1367         worst     0.150                   0.044                   0.194\n",
       "37            plot     0.142                   0.375                   0.517\n",
       "7             also     0.137                   0.604                   0.467\n",
       "144         script     0.129                   0.209                   0.338\n",
       "28           great     0.125                   0.411                   0.286\n",
       "21            best     0.124                   0.489                   0.365\n",
       "32           world     0.122                   0.363                   0.241\n",
       "1313        boring     0.121                   0.048                   0.169\n",
       "24            many     0.121                   0.455                   0.334\n",
       "133        nothing     0.118                   0.237                   0.355\n",
       "1483        stupid     0.116                   0.039                   0.155\n",
       "161   performances     0.110                   0.240                   0.130\n",
       "188        perfect     0.107                   0.182                   0.075"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catch['abs_diff'] = catch.apply(lambda x: np.abs(x[3] - x[6]), axis = 1)\n",
    "catch[['word', 'abs_diff', 'percent_containing_pos', 'percent_containing_neg']].sort_values('abs_diff', ascending = False).head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directional language is typically the top across absolute differences. Makes sense. Still, some other words are a signal (e.g. life, plot, script, and performances). Biopics may be the secret sauce, while bad writing is killer. What about relative differences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>rel_diff</th>\n",
       "      <th>percent_containing_pos</th>\n",
       "      <th>percent_containing_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3470</th>\n",
       "      <td>avoids</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3636</th>\n",
       "      <td>astounding</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3769</th>\n",
       "      <td>slip</td>\n",
       "      <td>0.941176</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3864</th>\n",
       "      <td>fascination</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26402</th>\n",
       "      <td>3000</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              word  rel_diff  percent_containing_pos  percent_containing_neg\n",
       "3470        avoids  0.947368                   0.019                   0.001\n",
       "3636    astounding  0.944444                   0.018                   0.001\n",
       "3769          slip  0.941176                   0.017                   0.001\n",
       "3864   fascination  0.937500                   0.016                   0.001\n",
       "26402         3000  0.937500                   0.001                   0.016"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catch['rel_diff'] = catch.apply(lambda x: np.abs(x[3] - x[6]) / max(x[3], x[6]), axis = 1)\n",
    "catch[['word', 'rel_diff', 'percent_containing_pos', 'percent_containing_neg']].sort_values('rel_diff', ascending = False).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This metric is dominated by small counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_of_interest = ['actors', 'writer', 'style', 'moment', 'performances', 'plot', 'script']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word:  actors\n",
      "\tPercent of mentions that are positive: 0.153\n",
      "\tPercent of mentions that are negative: 0.042\n",
      "Word:  writer\n",
      "\tPercent of mentions that are positive: 0.139\n",
      "\tPercent of mentions that are negative: 0.066\n",
      "Word:  style\n",
      "\tPercent of mentions that are positive: 0.114\n",
      "\tPercent of mentions that are negative: 0.042\n",
      "Word:  moment\n",
      "\tPercent of mentions that are positive: 0.088\n",
      "\tPercent of mentions that are negative: 0.039\n",
      "Word:  performances\n",
      "\tPercent of mentions that are positive: 0.231\n",
      "\tPercent of mentions that are negative: 0.043\n",
      "Word:  plot\n",
      "\tPercent of mentions that are positive: 0.069\n",
      "\tPercent of mentions that are negative: 0.053\n",
      "Word:  script\n",
      "\tPercent of mentions that are positive: 0.123\n",
      "\tPercent of mentions that are negative: 0.057\n"
     ]
    }
   ],
   "source": [
    "for word in words_of_interest:\n",
    "    sentiment_of_word(df, 'review_text', word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When reviewers are discussing actors and writers, there is a tendency to discuss them positively. Plot usage seems to be more balanced  We have a balanced positive / negative data set, but I wonder if usage in these reviews use words differently (i.e. more positive discussion in positive reviews)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word:  actors\n",
      "\tPercent of mentions that are positive: 0.196\n",
      "\tPercent of mentions that are negative: 0.028\n",
      "Word:  writer\n",
      "\tPercent of mentions that are positive: 0.181\n",
      "\tPercent of mentions that are negative: 0.028\n",
      "Word:  style\n",
      "\tPercent of mentions that are positive: 0.115\n",
      "\tPercent of mentions that are negative: 0.030\n",
      "Word:  moment\n",
      "\tPercent of mentions that are positive: 0.099\n",
      "\tPercent of mentions that are negative: 0.026\n",
      "Word:  performances\n",
      "\tPercent of mentions that are positive: 0.288\n",
      "\tPercent of mentions that are negative: 0.018\n",
      "Word:  plot\n",
      "\tPercent of mentions that are positive: 0.121\n",
      "\tPercent of mentions that are negative: 0.030\n",
      "Word:  script\n",
      "\tPercent of mentions that are positive: 0.213\n",
      "\tPercent of mentions that are negative: 0.031\n"
     ]
    }
   ],
   "source": [
    "for word in words_of_interest:\n",
    "    sentiment_of_word(df[df.rating=='pos'], 'review_text', word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word:  actors\n",
      "\tPercent of mentions that are positive: 0.112\n",
      "\tPercent of mentions that are negative: 0.055\n",
      "Word:  writer\n",
      "\tPercent of mentions that are positive: 0.101\n",
      "\tPercent of mentions that are negative: 0.101\n",
      "Word:  style\n",
      "\tPercent of mentions that are positive: 0.113\n",
      "\tPercent of mentions that are negative: 0.056\n",
      "Word:  moment\n",
      "\tPercent of mentions that are positive: 0.078\n",
      "\tPercent of mentions that are negative: 0.052\n",
      "Word:  performances\n",
      "\tPercent of mentions that are positive: 0.123\n",
      "\tPercent of mentions that are negative: 0.089\n",
      "Word:  plot\n",
      "\tPercent of mentions that are positive: 0.035\n",
      "\tPercent of mentions that are negative: 0.068\n",
      "Word:  script\n",
      "\tPercent of mentions that are positive: 0.067\n",
      "\tPercent of mentions that are negative: 0.074\n"
     ]
    }
   ],
   "source": [
    "for word in words_of_interest:\n",
    "    sentiment_of_word(df[df.rating=='neg'], 'review_text', word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evene among negative reviews, some words get used positively pretty often. Still, as expected, positive reviews are more positive in their discussion of these key topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words often travel in pairs (e.g. baseball bat). Are there such trends in the reviews?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigram</th>\n",
       "      <th>percent_containing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>special effects</td>\n",
       "      <td>0.1180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>even though</td>\n",
       "      <td>0.0955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>new york</td>\n",
       "      <td>0.0800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>looks like</td>\n",
       "      <td>0.0655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>one best</td>\n",
       "      <td>0.0630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208878</th>\n",
       "      <td>less willing</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208877</th>\n",
       "      <td>radical less</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208876</th>\n",
       "      <td>struggle radical</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208875</th>\n",
       "      <td>lose struggle</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>510018</th>\n",
       "      <td>left exactly</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>510019 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  bigram  percent_containing\n",
       "0        special effects              0.1180\n",
       "1            even though              0.0955\n",
       "2               new york              0.0800\n",
       "3             looks like              0.0655\n",
       "4               one best              0.0630\n",
       "...                  ...                 ...\n",
       "208878      less willing              0.0005\n",
       "208877      radical less              0.0005\n",
       "208876  struggle radical              0.0005\n",
       "208875     lose struggle              0.0005\n",
       "510018      left exactly              0.0005\n",
       "\n",
       "[510019 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_df = get_common_bigrams(df,'review_text')\n",
    "bigrams_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Special effects is the top bigram across all movies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigram</th>\n",
       "      <th>percent_containing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>special effects</td>\n",
       "      <td>0.112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>even though</td>\n",
       "      <td>0.105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>one best</td>\n",
       "      <td>0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>new york</td>\n",
       "      <td>0.084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>takes place</td>\n",
       "      <td>0.068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114278</th>\n",
       "      <td>costumes alexandra</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114279</th>\n",
       "      <td>byrne deserve</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114280</th>\n",
       "      <td>deserve mention</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114281</th>\n",
       "      <td>mention inportant</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288416</th>\n",
       "      <td>whatever may</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>288417 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    bigram  percent_containing\n",
       "0          special effects               0.112\n",
       "1              even though               0.105\n",
       "2                 one best               0.100\n",
       "3                 new york               0.084\n",
       "4              takes place               0.068\n",
       "...                    ...                 ...\n",
       "114278  costumes alexandra               0.001\n",
       "114279       byrne deserve               0.001\n",
       "114280     deserve mention               0.001\n",
       "114281   mention inportant               0.001\n",
       "288416        whatever may               0.001\n",
       "\n",
       "[288417 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_pos_df = get_common_bigrams(df[df.rating=='pos'],'review_text')\n",
    "bigrams_pos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigram</th>\n",
       "      <th>percent_containing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>special effects</td>\n",
       "      <td>0.124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>even though</td>\n",
       "      <td>0.086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>looks like</td>\n",
       "      <td>0.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>new york</td>\n",
       "      <td>0.076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>look like</td>\n",
       "      <td>0.065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100997</th>\n",
       "      <td>need bashed</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100998</th>\n",
       "      <td>bashed head</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100999</th>\n",
       "      <td>point bad</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101000</th>\n",
       "      <td>guys symbolic</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253916</th>\n",
       "      <td>left exactly</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>253917 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 bigram  percent_containing\n",
       "0       special effects               0.124\n",
       "1           even though               0.086\n",
       "2            looks like               0.083\n",
       "3              new york               0.076\n",
       "4             look like               0.065\n",
       "...                 ...                 ...\n",
       "100997      need bashed               0.001\n",
       "100998      bashed head               0.001\n",
       "100999        point bad               0.001\n",
       "101000    guys symbolic               0.001\n",
       "253916     left exactly               0.001\n",
       "\n",
       "[253917 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_neg_df = get_common_bigrams(df[df.rating=='neg'],'review_text')\n",
    "bigrams_neg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may be some subtle top-bigram differences in positively and negatively received movies. Are there any large differnces, like we saw in term mentions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "catch = bigrams_pos_df.merge(bigrams_neg_df, on = 'bigram', how = 'outer', suffixes = ['_pos', '_neg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "catch['abs_diff'] = catch.apply(lambda x: np.abs(x[1]-x[2]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigram</th>\n",
       "      <th>percent_containing_pos</th>\n",
       "      <th>percent_containing_neg</th>\n",
       "      <th>abs_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>one best</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>916</th>\n",
       "      <td>bad movie</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3468</th>\n",
       "      <td>waste time</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>looks like</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6320</th>\n",
       "      <td>one worst</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>film also</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>action sequences</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>best films</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>even worse</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>look like</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>young man</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>last year</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>throughout film</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2075</th>\n",
       "      <td>batman robin</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>much better</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>one day</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>good job</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>even better</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ever seen</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>866</th>\n",
       "      <td>five minutes</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>best performance</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>star wars</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>supporting cast</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>pulp fiction</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>action film</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>action scenes</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>best film</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>pretty much</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>title character</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>takes place</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                bigram  percent_containing_pos  percent_containing_neg  \\\n",
       "2             one best                   0.100                   0.026   \n",
       "916          bad movie                   0.009                   0.060   \n",
       "3468        waste time                   0.005                   0.046   \n",
       "18          looks like                   0.048                   0.083   \n",
       "6320         one worst                   0.003                   0.037   \n",
       "12           film also                   0.051                   0.018   \n",
       "88    action sequences                   0.029                   0.061   \n",
       "54          best films                   0.035                   0.006   \n",
       "473         even worse                   0.013                   0.042   \n",
       "46           look like                   0.037                   0.065   \n",
       "33           young man                   0.040                   0.012   \n",
       "50           last year                   0.037                   0.064   \n",
       "22     throughout film                   0.045                   0.018   \n",
       "2075      batman robin                   0.006                   0.032   \n",
       "67         much better                   0.032                   0.058   \n",
       "7              one day                   0.061                   0.036   \n",
       "36            good job                   0.039                   0.014   \n",
       "61         even better                   0.034                   0.009   \n",
       "6            ever seen                   0.061                   0.038   \n",
       "866       five minutes                   0.010                   0.033   \n",
       "80    best performance                   0.030                   0.007   \n",
       "27           star wars                   0.043                   0.020   \n",
       "5      supporting cast                   0.065                   0.043   \n",
       "60        pulp fiction                   0.034                   0.012   \n",
       "137        action film                   0.024                   0.046   \n",
       "153      action scenes                   0.023                   0.045   \n",
       "73           best film                   0.031                   0.009   \n",
       "72         pretty much                   0.031                   0.053   \n",
       "116    title character                   0.026                   0.005   \n",
       "4          takes place                   0.068                   0.048   \n",
       "\n",
       "      abs_diff  \n",
       "2        0.074  \n",
       "916      0.051  \n",
       "3468     0.041  \n",
       "18       0.035  \n",
       "6320     0.034  \n",
       "12       0.033  \n",
       "88       0.032  \n",
       "54       0.029  \n",
       "473      0.029  \n",
       "46       0.028  \n",
       "33       0.028  \n",
       "50       0.027  \n",
       "22       0.027  \n",
       "2075     0.026  \n",
       "67       0.026  \n",
       "7        0.025  \n",
       "36       0.025  \n",
       "61       0.025  \n",
       "6        0.023  \n",
       "866      0.023  \n",
       "80       0.023  \n",
       "27       0.023  \n",
       "5        0.022  \n",
       "60       0.022  \n",
       "137      0.022  \n",
       "153      0.022  \n",
       "73       0.022  \n",
       "72       0.022  \n",
       "116      0.021  \n",
       "4        0.020  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catch.sort_values('abs_diff', ascending = False).head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some directional language, some thematic insights, and some raw content (e.g. star wars)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Themes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some words travel in pairs, but some pairs (or groups of 3+ words) have similar meanings. Do such groups occur more / less in different films?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_theme = ['plot', 'storyline', 'story', 'narrative', 'events', 'story line', 'story', 'diegesis']\n",
    "comedy_theme = ['humor', 'humorous', 'funny', 'comedy', 'laugh', 'joke', 'goofy']\n",
    "\n",
    "themes = [plot_theme, comedy_theme]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theme words:  ['plot', 'storyline', 'story', 'narrative', 'events', 'story line', 'story', 'diegesis']\n",
      "Percent of entries containing theme: 0.813\n",
      "\n",
      "Theme words:  ['humor', 'humorous', 'funny', 'comedy', 'laugh', 'joke', 'goofy']\n",
      "Percent of entries containing theme: 0.578\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for theme in themes:\n",
    "    get_theme_frequency(df, 'review_text', theme)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mille\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:106: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theme words:  ['plot', 'storyline', 'story', 'narrative', 'events', 'story line', 'story', 'diegesis']\n",
      "Percent of entries containing theme: 0.793\n",
      "\n",
      "Theme words:  ['humor', 'humorous', 'funny', 'comedy', 'laugh', 'joke', 'goofy']\n",
      "Percent of entries containing theme: 0.530\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for theme in themes:\n",
    "    get_theme_frequency(df.loc[df.rating=='pos'], 'review_text', theme)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mille\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:106: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theme words:  ['plot', 'storyline', 'story', 'narrative', 'events', 'story line', 'story', 'diegesis']\n",
      "Percent of entries containing theme: 0.833\n",
      "\n",
      "Theme words:  ['humor', 'humorous', 'funny', 'comedy', 'laugh', 'joke', 'goofy']\n",
      "Percent of entries containing theme: 0.626\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for theme in themes:\n",
    "    get_theme_frequency(df.loc[df.rating=='neg'], 'review_text', theme)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion of plot transcends film quality. However, attempts at humor may be ill-received."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theme words:  ['plot', 'storyline', 'story', 'narrative', 'events', 'story line', 'story', 'diegesis']\n",
      "\tPercent of mentions that are positive: 0.102\n",
      "\tPercent of mentions that are negative: 0.055\n",
      "\n",
      "Theme words:  ['humor', 'humorous', 'funny', 'comedy', 'laugh', 'joke', 'goofy']\n",
      "\tPercent of mentions that are positive: 0.207\n",
      "\tPercent of mentions that are negative: 0.026\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for theme in themes:\n",
    "    sentiment_of_theme(df, 'review_text', theme)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While comedy occurs more in negative reviews, usage of this theme is typically positive at the highest level of aggregation. Do differences emerge when we filter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theme words:  ['plot', 'storyline', 'story', 'narrative', 'events', 'story line', 'story', 'diegesis']\n",
      "\tPercent of mentions that are positive: 0.143\n",
      "\tPercent of mentions that are negative: 0.045\n",
      "\n",
      "Theme words:  ['humor', 'humorous', 'funny', 'comedy', 'laugh', 'joke', 'goofy']\n",
      "\tPercent of mentions that are positive: 0.300\n",
      "\tPercent of mentions that are negative: 0.019\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for theme in themes:\n",
    "    sentiment_of_theme(df.loc[df.rating=='pos'], 'review_text', theme)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theme words:  ['plot', 'storyline', 'story', 'narrative', 'events', 'story line', 'story', 'diegesis']\n",
      "\tPercent of mentions that are positive: 0.058\n",
      "\tPercent of mentions that are negative: 0.066\n",
      "\n",
      "Theme words:  ['humor', 'humorous', 'funny', 'comedy', 'laugh', 'joke', 'goofy']\n",
      "\tPercent of mentions that are positive: 0.132\n",
      "\tPercent of mentions that are negative: 0.031\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for theme in themes:\n",
    "    sentiment_of_theme(df.loc[df.rating=='neg'], 'review_text', theme)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differences do, in fact, emerge. However, within negative reviews, a lot of mentions are still positive. This may be because we are using a built in sentiment analyzer and comedic themes are typically positive. Differneces across positive and negative review groups are insightful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we leverage the language used in review texts to identify positive and negative reviews?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>abs_diff</th>\n",
       "      <th>percent_containing_pos</th>\n",
       "      <th>percent_containing_neg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>bad</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>life</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1367</th>\n",
       "      <td>worst</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>plot</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>also</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.604</td>\n",
       "      <td>0.467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>script</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>great</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>best</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.489</td>\n",
       "      <td>0.365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>world</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1313</th>\n",
       "      <td>boring</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>many</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>nothing</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1483</th>\n",
       "      <td>stupid</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>performances</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>perfect</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              word  abs_diff  percent_containing_pos  percent_containing_neg\n",
       "108            bad     0.255                   0.259                   0.514\n",
       "11            life     0.158                   0.492                   0.334\n",
       "1367         worst     0.150                   0.044                   0.194\n",
       "37            plot     0.142                   0.375                   0.517\n",
       "7             also     0.137                   0.604                   0.467\n",
       "144         script     0.129                   0.209                   0.338\n",
       "28           great     0.125                   0.411                   0.286\n",
       "21            best     0.124                   0.489                   0.365\n",
       "32           world     0.122                   0.363                   0.241\n",
       "1313        boring     0.121                   0.048                   0.169\n",
       "24            many     0.121                   0.455                   0.334\n",
       "133        nothing     0.118                   0.237                   0.355\n",
       "1483        stupid     0.116                   0.039                   0.155\n",
       "161   performances     0.110                   0.240                   0.130\n",
       "188        perfect     0.107                   0.182                   0.075"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catch = term_freqs_pos.merge(term_freqs_neg, on = 'word', how = 'outer', suffixes = ['_pos', '_neg'])\n",
    "catch['abs_diff'] = catch.apply(lambda x: np.abs(x[3] - x[6]), axis = 1)\n",
    "catch[['word', 'abs_diff', 'percent_containing_pos', 'percent_containing_neg']].sort_values('abs_diff', ascending = False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_features = catch.sort_values('abs_diff', ascending = False).head(100).word.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def review_features(review):\n",
    "    tokens = nltk.word_tokenize(review)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains_{}'.format(word)] = (word in tokens)\n",
    "    return features\n",
    "\n",
    "df['tokenized'] = df.review_text.apply(lambda x: nltk.word_tokenize(x))\n",
    "\n",
    "temp = df.copy()\n",
    "\n",
    "for word in word_features:\n",
    "        temp['contains_{}'.format(word)] = temp.review_text.str.contains(word)\n",
    "\n",
    "X = temp.iloc[:,5:]\n",
    "y = temp['rating']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7893939393939394"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "confusion_matrix(y_test, y_pred)\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a balanced data set, this not too shabby. Do other models with the same features do any better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.317750\n",
      "         Iterations 8\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "from statsmodels.discrete.discrete_model import Logit\n",
    "\n",
    "temp = df.copy()\n",
    "for word in word_features:\n",
    "        temp['contains_{}'.format(word)] = temp.review_text.str.contains(word)\n",
    "\n",
    "X = temp.iloc[:,5:]\n",
    "X = sm.add_constant(X)\n",
    "y = (temp['rating']=='pos')\n",
    "\n",
    "y = np.array(y, dtype=float)\n",
    "X = np.array(X, dtype=float)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "lr = Logit(y_train, X_train).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8196969696969697"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = lr.predict(X_test) > .5\n",
    "\n",
    "confusion_matrix(y_test, y_pred)\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression edges out Naiive Bayes! We also get to see how individual terms influence outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>odds_ratio</th>\n",
       "      <th>variable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>5.114084</td>\n",
       "      <td>contains_memorable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>3.037992</td>\n",
       "      <td>contains_solid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>3.013555</td>\n",
       "      <td>contains_excellent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2.999779</td>\n",
       "      <td>contains_hilarious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>2.778813</td>\n",
       "      <td>contains_subtle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>2.697125</td>\n",
       "      <td>contains_sometimes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2.523307</td>\n",
       "      <td>contains_perfectly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2.452022</td>\n",
       "      <td>contains_wonderful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2.392911</td>\n",
       "      <td>contains_quite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>2.258487</td>\n",
       "      <td>contains_brilliant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2.055095</td>\n",
       "      <td>contains_others</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>2.013473</td>\n",
       "      <td>contains_seen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.937280</td>\n",
       "      <td>contains_performances</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1.879181</td>\n",
       "      <td>contains_works</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>1.824414</td>\n",
       "      <td>contains_simple</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    odds_ratio               variable\n",
       "48    5.114084     contains_memorable\n",
       "78    3.037992         contains_solid\n",
       "47    3.013555     contains_excellent\n",
       "50    2.999779     contains_hilarious\n",
       "66    2.778813        contains_subtle\n",
       "73    2.697125     contains_sometimes\n",
       "49    2.523307     contains_perfectly\n",
       "40    2.452022     contains_wonderful\n",
       "28    2.392911         contains_quite\n",
       "69    2.258487     contains_brilliant\n",
       "32    2.055095        contains_others\n",
       "57    2.013473          contains_seen\n",
       "14    1.937280  contains_performances\n",
       "52    1.879181         contains_works\n",
       "89    1.824414        contains_simple"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = temp.iloc[:,5:]\n",
    "X = sm.add_constant(X)\n",
    "res_df=pd.DataFrame({'odds_ratio':(np.exp(lr.params).T).tolist(),'variable':X.columns.tolist()})\n",
    "res_df=res_df.sort_values('odds_ratio', ascending=False)\n",
    "\n",
    "pd.options.display.max_rows = 63\n",
    "res_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>odds_ratio</th>\n",
       "      <th>variable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.447695</td>\n",
       "      <td>contains_attempt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.425336</td>\n",
       "      <td>contains_terrible</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>0.417692</td>\n",
       "      <td>contains_poorly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>0.392694</td>\n",
       "      <td>contains_poor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.390138</td>\n",
       "      <td>contains_dull</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.382662</td>\n",
       "      <td>contains_worst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.375660</td>\n",
       "      <td>contains_nothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.364639</td>\n",
       "      <td>contains_wasted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.324643</td>\n",
       "      <td>contains_unfortunately</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.303092</td>\n",
       "      <td>contains_supposed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.277436</td>\n",
       "      <td>contains_stupid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.264811</td>\n",
       "      <td>contains_lame</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.251370</td>\n",
       "      <td>contains_boring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.192694</td>\n",
       "      <td>contains_ridiculous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.157306</td>\n",
       "      <td>contains_awful</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    odds_ratio                variable\n",
       "63    0.447695        contains_attempt\n",
       "70    0.425336       contains_terrible\n",
       "91    0.417692         contains_poorly\n",
       "77    0.392694           contains_poor\n",
       "60    0.390138           contains_dull\n",
       "3     0.382662          contains_worst\n",
       "12    0.375660        contains_nothing\n",
       "41    0.364639         contains_wasted\n",
       "20    0.324643  contains_unfortunately\n",
       "18    0.303092       contains_supposed\n",
       "13    0.277436         contains_stupid\n",
       "61    0.264811           contains_lame\n",
       "10    0.251370         contains_boring\n",
       "27    0.192694     contains_ridiculous\n",
       "33    0.157306          contains_awful"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df.tail(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directional language is most insightful. Nothing too wild."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['const'] + temp.iloc[:,5:].columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets train again, using all of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.338146\n",
      "         Iterations 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>y</td>        <th>  No. Observations:  </th>   <td>  2000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>  1899</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>   100</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Sun, 24 Oct 2021</td> <th>  Pseudo R-squ.:     </th>   <td>0.5122</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>10:57:32</td>     <th>  Log-Likelihood:    </th>  <td> -676.29</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th>  <td> -1386.3</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>4.058e-232</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "             <td></td>               <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>                  <td>   -0.6115</td> <td>    0.268</td> <td>   -2.282</td> <td> 0.022</td> <td>   -1.137</td> <td>   -0.086</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_bad</th>           <td>   -0.6624</td> <td>    0.154</td> <td>   -4.296</td> <td> 0.000</td> <td>   -0.965</td> <td>   -0.360</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_life</th>          <td>    0.1157</td> <td>    0.147</td> <td>    0.786</td> <td> 0.432</td> <td>   -0.173</td> <td>    0.404</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_worst</th>         <td>   -1.1886</td> <td>    0.276</td> <td>   -4.310</td> <td> 0.000</td> <td>   -1.729</td> <td>   -0.648</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_plot</th>          <td>   -0.5559</td> <td>    0.148</td> <td>   -3.764</td> <td> 0.000</td> <td>   -0.845</td> <td>   -0.266</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_also</th>          <td>    0.5085</td> <td>    0.147</td> <td>    3.455</td> <td> 0.001</td> <td>    0.220</td> <td>    0.797</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_script</th>        <td>   -0.4893</td> <td>    0.159</td> <td>   -3.068</td> <td> 0.002</td> <td>   -0.802</td> <td>   -0.177</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_great</th>         <td>    0.4952</td> <td>    0.153</td> <td>    3.236</td> <td> 0.001</td> <td>    0.195</td> <td>    0.795</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_best</th>          <td>    0.2739</td> <td>    0.149</td> <td>    1.844</td> <td> 0.065</td> <td>   -0.017</td> <td>    0.565</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_world</th>         <td>    0.3316</td> <td>    0.161</td> <td>    2.060</td> <td> 0.039</td> <td>    0.016</td> <td>    0.647</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_boring</th>        <td>   -1.1206</td> <td>    0.255</td> <td>   -4.391</td> <td> 0.000</td> <td>   -1.621</td> <td>   -0.620</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_many</th>          <td>    0.3637</td> <td>    0.154</td> <td>    2.364</td> <td> 0.018</td> <td>    0.062</td> <td>    0.665</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_nothing</th>       <td>   -0.8806</td> <td>    0.165</td> <td>   -5.323</td> <td> 0.000</td> <td>   -1.205</td> <td>   -0.556</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_stupid</th>        <td>   -1.1371</td> <td>    0.279</td> <td>   -4.082</td> <td> 0.000</td> <td>   -1.683</td> <td>   -0.591</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_performances</th>  <td>    0.6753</td> <td>    0.241</td> <td>    2.802</td> <td> 0.005</td> <td>    0.203</td> <td>    1.148</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_perfect</th>       <td>    0.4825</td> <td>    0.220</td> <td>    2.197</td> <td> 0.028</td> <td>    0.052</td> <td>    0.913</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_performance</th>   <td>    0.0455</td> <td>    0.192</td> <td>    0.237</td> <td> 0.813</td> <td>   -0.331</td> <td>    0.422</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_least</th>         <td>   -0.2751</td> <td>    0.170</td> <td>   -1.621</td> <td> 0.105</td> <td>   -0.608</td> <td>    0.058</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_supposed</th>      <td>   -1.1636</td> <td>    0.209</td> <td>   -5.572</td> <td> 0.000</td> <td>   -1.573</td> <td>   -0.754</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_true</th>          <td>    0.5177</td> <td>    0.195</td> <td>    2.656</td> <td> 0.008</td> <td>    0.136</td> <td>    0.900</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_unfortunately</th> <td>   -1.0034</td> <td>    0.209</td> <td>   -4.812</td> <td> 0.000</td> <td>   -1.412</td> <td>   -0.595</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_looks</th>         <td>   -0.5233</td> <td>    0.189</td> <td>   -2.770</td> <td> 0.006</td> <td>   -0.894</td> <td>   -0.153</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_especially</th>    <td>    0.5654</td> <td>    0.196</td> <td>    2.878</td> <td> 0.004</td> <td>    0.180</td> <td>    0.950</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_american</th>      <td>    0.3349</td> <td>    0.190</td> <td>    1.758</td> <td> 0.079</td> <td>   -0.038</td> <td>    0.708</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_different</th>     <td>    0.3258</td> <td>    0.187</td> <td>    1.740</td> <td> 0.082</td> <td>   -0.041</td> <td>    0.693</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_family</th>        <td>    0.3769</td> <td>    0.197</td> <td>    1.910</td> <td> 0.056</td> <td>   -0.010</td> <td>    0.763</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_waste</th>         <td>   -1.0672</td> <td>    0.319</td> <td>   -3.342</td> <td> 0.001</td> <td>   -1.693</td> <td>   -0.441</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_ridiculous</th>    <td>   -1.5794</td> <td>    0.317</td> <td>   -4.986</td> <td> 0.000</td> <td>   -2.200</td> <td>   -0.959</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_quite</th>         <td>    0.5645</td> <td>    0.176</td> <td>    3.212</td> <td> 0.001</td> <td>    0.220</td> <td>    0.909</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_mess</th>          <td>   -0.5589</td> <td>    0.203</td> <td>   -2.758</td> <td> 0.006</td> <td>   -0.956</td> <td>   -0.162</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_reason</th>        <td>   -0.5421</td> <td>    0.174</td> <td>   -3.120</td> <td> 0.002</td> <td>   -0.883</td> <td>   -0.202</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_well</th>          <td>    0.1474</td> <td>    0.148</td> <td>    0.995</td> <td> 0.320</td> <td>   -0.143</td> <td>    0.438</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_others</th>        <td>    0.3153</td> <td>    0.183</td> <td>    1.722</td> <td> 0.085</td> <td>   -0.044</td> <td>    0.674</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_awful</th>         <td>   -1.2155</td> <td>    0.312</td> <td>   -3.899</td> <td> 0.000</td> <td>   -1.826</td> <td>   -0.605</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_although</th>      <td>    0.4659</td> <td>    0.172</td> <td>    2.709</td> <td> 0.007</td> <td>    0.129</td> <td>    0.803</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_maybe</th>         <td>   -0.7257</td> <td>    0.253</td> <td>   -2.864</td> <td> 0.004</td> <td>   -1.222</td> <td>   -0.229</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_may</th>           <td>    0.0054</td> <td>    0.171</td> <td>    0.031</td> <td> 0.975</td> <td>   -0.331</td> <td>    0.341</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_gives</th>         <td>    0.3958</td> <td>    0.225</td> <td>    1.759</td> <td> 0.079</td> <td>   -0.045</td> <td>    0.837</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_young</th>         <td>    0.1098</td> <td>    0.164</td> <td>    0.668</td> <td> 0.504</td> <td>   -0.212</td> <td>    0.432</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_yet</th>           <td>    0.3580</td> <td>    0.175</td> <td>    2.040</td> <td> 0.041</td> <td>    0.014</td> <td>    0.702</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_wonderful</th>     <td>    0.9165</td> <td>    0.255</td> <td>    3.589</td> <td> 0.000</td> <td>    0.416</td> <td>    1.417</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_wasted</th>        <td>   -0.2613</td> <td>    0.474</td> <td>   -0.551</td> <td> 0.582</td> <td>   -1.191</td> <td>    0.668</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_worse</th>         <td>   -0.4571</td> <td>    0.253</td> <td>   -1.808</td> <td> 0.071</td> <td>   -0.953</td> <td>    0.039</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_years</th>         <td>   -0.1205</td> <td>    0.161</td> <td>   -0.748</td> <td> 0.455</td> <td>   -0.436</td> <td>    0.195</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_effective</th>     <td>    0.1821</td> <td>    0.238</td> <td>    0.767</td> <td> 0.443</td> <td>   -0.283</td> <td>    0.648</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_job</th>           <td>    0.3532</td> <td>    0.172</td> <td>    2.049</td> <td> 0.040</td> <td>    0.015</td> <td>    0.691</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_movie</th>         <td>    0.0543</td> <td>    0.187</td> <td>    0.290</td> <td> 0.772</td> <td>   -0.312</td> <td>    0.421</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_excellent</th>     <td>    0.9964</td> <td>    0.305</td> <td>    3.272</td> <td> 0.001</td> <td>    0.400</td> <td>    1.593</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_memorable</th>     <td>    1.3844</td> <td>    0.347</td> <td>    3.995</td> <td> 0.000</td> <td>    0.705</td> <td>    2.064</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_perfectly</th>     <td>    0.7690</td> <td>    0.367</td> <td>    2.093</td> <td> 0.036</td> <td>    0.049</td> <td>    1.489</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_hilarious</th>     <td>    0.9490</td> <td>    0.257</td> <td>    3.686</td> <td> 0.000</td> <td>    0.444</td> <td>    1.454</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_takes</th>         <td>    0.1645</td> <td>    0.164</td> <td>    1.003</td> <td> 0.316</td> <td>   -0.157</td> <td>    0.486</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_works</th>         <td>    0.6034</td> <td>    0.200</td> <td>    3.013</td> <td> 0.003</td> <td>    0.211</td> <td>    0.996</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_makes</th>         <td>    0.3698</td> <td>    0.152</td> <td>    2.426</td> <td> 0.015</td> <td>    0.071</td> <td>    0.669</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_always</th>        <td>    0.2774</td> <td>    0.181</td> <td>    1.535</td> <td> 0.125</td> <td>   -0.077</td> <td>    0.632</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_see</th>           <td>   -0.3413</td> <td>    0.192</td> <td>   -1.781</td> <td> 0.075</td> <td>   -0.717</td> <td>    0.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_strong</th>        <td>    0.3597</td> <td>    0.215</td> <td>    1.672</td> <td> 0.094</td> <td>   -0.062</td> <td>    0.781</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_seen</th>          <td>    0.6523</td> <td>    0.171</td> <td>    3.823</td> <td> 0.000</td> <td>    0.318</td> <td>    0.987</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_none</th>          <td>   -0.4703</td> <td>    0.217</td> <td>   -2.169</td> <td> 0.030</td> <td>   -0.895</td> <td>   -0.045</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_minutes</th>       <td>    0.1832</td> <td>    0.329</td> <td>    0.557</td> <td> 0.577</td> <td>   -0.461</td> <td>    0.828</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_dull</th>          <td>   -1.2539</td> <td>    0.356</td> <td>   -3.526</td> <td> 0.000</td> <td>   -1.951</td> <td>   -0.557</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_lame</th>          <td>   -0.8394</td> <td>    0.248</td> <td>   -3.380</td> <td> 0.001</td> <td>   -1.326</td> <td>   -0.353</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_begins</th>        <td>    0.2428</td> <td>    0.205</td> <td>    1.183</td> <td> 0.237</td> <td>   -0.159</td> <td>    0.645</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_attempt</th>       <td>   -0.6836</td> <td>    0.186</td> <td>   -3.666</td> <td> 0.000</td> <td>   -1.049</td> <td>   -0.318</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_shows</th>         <td>    0.2375</td> <td>    0.202</td> <td>    1.174</td> <td> 0.240</td> <td>   -0.159</td> <td>    0.634</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_minute</th>        <td>   -0.4455</td> <td>    0.305</td> <td>   -1.460</td> <td> 0.144</td> <td>   -1.043</td> <td>    0.153</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_subtle</th>        <td>    0.5340</td> <td>    0.290</td> <td>    1.843</td> <td> 0.065</td> <td>   -0.034</td> <td>    1.102</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_got</th>           <td>   -0.0300</td> <td>    0.170</td> <td>   -0.176</td> <td> 0.860</td> <td>   -0.363</td> <td>    0.303</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_dialogue</th>      <td>   -0.0663</td> <td>    0.194</td> <td>   -0.342</td> <td> 0.733</td> <td>   -0.447</td> <td>    0.314</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_brilliant</th>     <td>    0.5194</td> <td>    0.279</td> <td>    1.865</td> <td> 0.062</td> <td>   -0.027</td> <td>    1.065</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_terrible</th>      <td>   -1.0532</td> <td>    0.339</td> <td>   -3.105</td> <td> 0.002</td> <td>   -1.718</td> <td>   -0.388</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_become</th>        <td>    0.0569</td> <td>    0.157</td> <td>    0.362</td> <td> 0.718</td> <td>   -0.251</td> <td>    0.365</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_films</th>         <td>    0.2212</td> <td>    0.152</td> <td>    1.456</td> <td> 0.145</td> <td>   -0.077</td> <td>    0.519</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_sometimes</th>     <td>    0.9536</td> <td>    0.243</td> <td>    3.919</td> <td> 0.000</td> <td>    0.477</td> <td>    1.430</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_better</th>        <td>   -0.2967</td> <td>    0.161</td> <td>   -1.846</td> <td> 0.065</td> <td>   -0.612</td> <td>    0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_oscar</th>         <td>    0.5200</td> <td>    0.257</td> <td>    2.026</td> <td> 0.043</td> <td>    0.017</td> <td>    1.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_throughout</th>    <td>    0.5656</td> <td>    0.230</td> <td>    2.464</td> <td> 0.014</td> <td>    0.116</td> <td>    1.016</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_poor</th>          <td>   -0.6402</td> <td>    0.281</td> <td>   -2.278</td> <td> 0.023</td> <td>   -1.191</td> <td>   -0.089</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_solid</th>         <td>    0.7749</td> <td>    0.292</td> <td>    2.652</td> <td> 0.008</td> <td>    0.202</td> <td>    1.348</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_father</th>        <td>   -0.0355</td> <td>    0.195</td> <td>   -0.182</td> <td> 0.856</td> <td>   -0.418</td> <td>    0.347</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_finds</th>         <td>    0.2002</td> <td>    0.212</td> <td>    0.943</td> <td> 0.346</td> <td>   -0.216</td> <td>    0.616</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_guess</th>         <td>   -0.5401</td> <td>    0.232</td> <td>   -2.331</td> <td> 0.020</td> <td>   -0.994</td> <td>   -0.086</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_pretty</th>        <td>   -0.0900</td> <td>    0.191</td> <td>   -0.472</td> <td> 0.637</td> <td>   -0.464</td> <td>    0.284</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_mother</th>        <td>    0.1799</td> <td>    0.212</td> <td>    0.847</td> <td> 0.397</td> <td>   -0.236</td> <td>    0.596</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_history</th>       <td>    0.7288</td> <td>    0.287</td> <td>    2.536</td> <td> 0.011</td> <td>    0.166</td> <td>    1.292</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_right</th>         <td>    0.1505</td> <td>    0.149</td> <td>    1.008</td> <td> 0.313</td> <td>   -0.142</td> <td>    0.443</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_fails</th>         <td>   -0.7613</td> <td>    0.313</td> <td>   -2.434</td> <td> 0.015</td> <td>   -1.374</td> <td>   -0.148</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_war</th>           <td>    0.1735</td> <td>    0.149</td> <td>    1.167</td> <td> 0.243</td> <td>   -0.118</td> <td>    0.465</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_tries</th>         <td>   -0.6702</td> <td>    0.190</td> <td>   -3.521</td> <td> 0.000</td> <td>   -1.043</td> <td>   -0.297</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_simple</th>        <td>    0.6063</td> <td>    0.223</td> <td>    2.713</td> <td> 0.007</td> <td>    0.168</td> <td>    1.044</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_human</th>         <td>    0.3591</td> <td>    0.186</td> <td>    1.931</td> <td> 0.053</td> <td>   -0.005</td> <td>    0.724</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_poorly</th>        <td>   -0.7574</td> <td>    0.531</td> <td>   -1.426</td> <td> 0.154</td> <td>   -1.798</td> <td>    0.283</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_small</th>         <td>    0.0118</td> <td>    0.190</td> <td>    0.062</td> <td> 0.950</td> <td>   -0.360</td> <td>    0.384</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_extremely</th>     <td>    0.8683</td> <td>    0.278</td> <td>    3.120</td> <td> 0.002</td> <td>    0.323</td> <td>    1.414</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_outstanding</th>   <td>    1.6498</td> <td>    0.589</td> <td>    2.803</td> <td> 0.005</td> <td>    0.496</td> <td>    2.803</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_save</th>          <td>   -0.6067</td> <td>    0.204</td> <td>   -2.967</td> <td> 0.003</td> <td>   -1.007</td> <td>   -0.206</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_give</th>          <td>   -0.2293</td> <td>    0.167</td> <td>   -1.377</td> <td> 0.169</td> <td>   -0.556</td> <td>    0.097</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_ending</th>        <td>    0.1973</td> <td>    0.178</td> <td>    1.108</td> <td> 0.268</td> <td>   -0.152</td> <td>    0.546</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_still</th>         <td>    0.2818</td> <td>    0.154</td> <td>    1.828</td> <td> 0.068</td> <td>   -0.020</td> <td>    0.584</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_people</th>        <td>    0.4420</td> <td>    0.152</td> <td>    2.910</td> <td> 0.004</td> <td>    0.144</td> <td>    0.740</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>contains_love</th>          <td>   -0.0504</td> <td>    0.146</td> <td>   -0.345</td> <td> 0.730</td> <td>   -0.337</td> <td>    0.236</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   No. Observations:                 2000\n",
       "Model:                          Logit   Df Residuals:                     1899\n",
       "Method:                           MLE   Df Model:                          100\n",
       "Date:                Sun, 24 Oct 2021   Pseudo R-squ.:                  0.5122\n",
       "Time:                        10:57:32   Log-Likelihood:                -676.29\n",
       "converged:                       True   LL-Null:                       -1386.3\n",
       "Covariance Type:            nonrobust   LLR p-value:                4.058e-232\n",
       "==========================================================================================\n",
       "                             coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------------\n",
       "const                     -0.6115      0.268     -2.282      0.022      -1.137      -0.086\n",
       "contains_bad              -0.6624      0.154     -4.296      0.000      -0.965      -0.360\n",
       "contains_life              0.1157      0.147      0.786      0.432      -0.173       0.404\n",
       "contains_worst            -1.1886      0.276     -4.310      0.000      -1.729      -0.648\n",
       "contains_plot             -0.5559      0.148     -3.764      0.000      -0.845      -0.266\n",
       "contains_also              0.5085      0.147      3.455      0.001       0.220       0.797\n",
       "contains_script           -0.4893      0.159     -3.068      0.002      -0.802      -0.177\n",
       "contains_great             0.4952      0.153      3.236      0.001       0.195       0.795\n",
       "contains_best              0.2739      0.149      1.844      0.065      -0.017       0.565\n",
       "contains_world             0.3316      0.161      2.060      0.039       0.016       0.647\n",
       "contains_boring           -1.1206      0.255     -4.391      0.000      -1.621      -0.620\n",
       "contains_many              0.3637      0.154      2.364      0.018       0.062       0.665\n",
       "contains_nothing          -0.8806      0.165     -5.323      0.000      -1.205      -0.556\n",
       "contains_stupid           -1.1371      0.279     -4.082      0.000      -1.683      -0.591\n",
       "contains_performances      0.6753      0.241      2.802      0.005       0.203       1.148\n",
       "contains_perfect           0.4825      0.220      2.197      0.028       0.052       0.913\n",
       "contains_performance       0.0455      0.192      0.237      0.813      -0.331       0.422\n",
       "contains_least            -0.2751      0.170     -1.621      0.105      -0.608       0.058\n",
       "contains_supposed         -1.1636      0.209     -5.572      0.000      -1.573      -0.754\n",
       "contains_true              0.5177      0.195      2.656      0.008       0.136       0.900\n",
       "contains_unfortunately    -1.0034      0.209     -4.812      0.000      -1.412      -0.595\n",
       "contains_looks            -0.5233      0.189     -2.770      0.006      -0.894      -0.153\n",
       "contains_especially        0.5654      0.196      2.878      0.004       0.180       0.950\n",
       "contains_american          0.3349      0.190      1.758      0.079      -0.038       0.708\n",
       "contains_different         0.3258      0.187      1.740      0.082      -0.041       0.693\n",
       "contains_family            0.3769      0.197      1.910      0.056      -0.010       0.763\n",
       "contains_waste            -1.0672      0.319     -3.342      0.001      -1.693      -0.441\n",
       "contains_ridiculous       -1.5794      0.317     -4.986      0.000      -2.200      -0.959\n",
       "contains_quite             0.5645      0.176      3.212      0.001       0.220       0.909\n",
       "contains_mess             -0.5589      0.203     -2.758      0.006      -0.956      -0.162\n",
       "contains_reason           -0.5421      0.174     -3.120      0.002      -0.883      -0.202\n",
       "contains_well              0.1474      0.148      0.995      0.320      -0.143       0.438\n",
       "contains_others            0.3153      0.183      1.722      0.085      -0.044       0.674\n",
       "contains_awful            -1.2155      0.312     -3.899      0.000      -1.826      -0.605\n",
       "contains_although          0.4659      0.172      2.709      0.007       0.129       0.803\n",
       "contains_maybe            -0.7257      0.253     -2.864      0.004      -1.222      -0.229\n",
       "contains_may               0.0054      0.171      0.031      0.975      -0.331       0.341\n",
       "contains_gives             0.3958      0.225      1.759      0.079      -0.045       0.837\n",
       "contains_young             0.1098      0.164      0.668      0.504      -0.212       0.432\n",
       "contains_yet               0.3580      0.175      2.040      0.041       0.014       0.702\n",
       "contains_wonderful         0.9165      0.255      3.589      0.000       0.416       1.417\n",
       "contains_wasted           -0.2613      0.474     -0.551      0.582      -1.191       0.668\n",
       "contains_worse            -0.4571      0.253     -1.808      0.071      -0.953       0.039\n",
       "contains_years            -0.1205      0.161     -0.748      0.455      -0.436       0.195\n",
       "contains_effective         0.1821      0.238      0.767      0.443      -0.283       0.648\n",
       "contains_job               0.3532      0.172      2.049      0.040       0.015       0.691\n",
       "contains_movie             0.0543      0.187      0.290      0.772      -0.312       0.421\n",
       "contains_excellent         0.9964      0.305      3.272      0.001       0.400       1.593\n",
       "contains_memorable         1.3844      0.347      3.995      0.000       0.705       2.064\n",
       "contains_perfectly         0.7690      0.367      2.093      0.036       0.049       1.489\n",
       "contains_hilarious         0.9490      0.257      3.686      0.000       0.444       1.454\n",
       "contains_takes             0.1645      0.164      1.003      0.316      -0.157       0.486\n",
       "contains_works             0.6034      0.200      3.013      0.003       0.211       0.996\n",
       "contains_makes             0.3698      0.152      2.426      0.015       0.071       0.669\n",
       "contains_always            0.2774      0.181      1.535      0.125      -0.077       0.632\n",
       "contains_see              -0.3413      0.192     -1.781      0.075      -0.717       0.034\n",
       "contains_strong            0.3597      0.215      1.672      0.094      -0.062       0.781\n",
       "contains_seen              0.6523      0.171      3.823      0.000       0.318       0.987\n",
       "contains_none             -0.4703      0.217     -2.169      0.030      -0.895      -0.045\n",
       "contains_minutes           0.1832      0.329      0.557      0.577      -0.461       0.828\n",
       "contains_dull             -1.2539      0.356     -3.526      0.000      -1.951      -0.557\n",
       "contains_lame             -0.8394      0.248     -3.380      0.001      -1.326      -0.353\n",
       "contains_begins            0.2428      0.205      1.183      0.237      -0.159       0.645\n",
       "contains_attempt          -0.6836      0.186     -3.666      0.000      -1.049      -0.318\n",
       "contains_shows             0.2375      0.202      1.174      0.240      -0.159       0.634\n",
       "contains_minute           -0.4455      0.305     -1.460      0.144      -1.043       0.153\n",
       "contains_subtle            0.5340      0.290      1.843      0.065      -0.034       1.102\n",
       "contains_got              -0.0300      0.170     -0.176      0.860      -0.363       0.303\n",
       "contains_dialogue         -0.0663      0.194     -0.342      0.733      -0.447       0.314\n",
       "contains_brilliant         0.5194      0.279      1.865      0.062      -0.027       1.065\n",
       "contains_terrible         -1.0532      0.339     -3.105      0.002      -1.718      -0.388\n",
       "contains_become            0.0569      0.157      0.362      0.718      -0.251       0.365\n",
       "contains_films             0.2212      0.152      1.456      0.145      -0.077       0.519\n",
       "contains_sometimes         0.9536      0.243      3.919      0.000       0.477       1.430\n",
       "contains_better           -0.2967      0.161     -1.846      0.065      -0.612       0.018\n",
       "contains_oscar             0.5200      0.257      2.026      0.043       0.017       1.023\n",
       "contains_throughout        0.5656      0.230      2.464      0.014       0.116       1.016\n",
       "contains_poor             -0.6402      0.281     -2.278      0.023      -1.191      -0.089\n",
       "contains_solid             0.7749      0.292      2.652      0.008       0.202       1.348\n",
       "contains_father           -0.0355      0.195     -0.182      0.856      -0.418       0.347\n",
       "contains_finds             0.2002      0.212      0.943      0.346      -0.216       0.616\n",
       "contains_guess            -0.5401      0.232     -2.331      0.020      -0.994      -0.086\n",
       "contains_pretty           -0.0900      0.191     -0.472      0.637      -0.464       0.284\n",
       "contains_mother            0.1799      0.212      0.847      0.397      -0.236       0.596\n",
       "contains_history           0.7288      0.287      2.536      0.011       0.166       1.292\n",
       "contains_right             0.1505      0.149      1.008      0.313      -0.142       0.443\n",
       "contains_fails            -0.7613      0.313     -2.434      0.015      -1.374      -0.148\n",
       "contains_war               0.1735      0.149      1.167      0.243      -0.118       0.465\n",
       "contains_tries            -0.6702      0.190     -3.521      0.000      -1.043      -0.297\n",
       "contains_simple            0.6063      0.223      2.713      0.007       0.168       1.044\n",
       "contains_human             0.3591      0.186      1.931      0.053      -0.005       0.724\n",
       "contains_poorly           -0.7574      0.531     -1.426      0.154      -1.798       0.283\n",
       "contains_small             0.0118      0.190      0.062      0.950      -0.360       0.384\n",
       "contains_extremely         0.8683      0.278      3.120      0.002       0.323       1.414\n",
       "contains_outstanding       1.6498      0.589      2.803      0.005       0.496       2.803\n",
       "contains_save             -0.6067      0.204     -2.967      0.003      -1.007      -0.206\n",
       "contains_give             -0.2293      0.167     -1.377      0.169      -0.556       0.097\n",
       "contains_ending            0.1973      0.178      1.108      0.268      -0.152       0.546\n",
       "contains_still             0.2818      0.154      1.828      0.068      -0.020       0.584\n",
       "contains_people            0.4420      0.152      2.910      0.004       0.144       0.740\n",
       "contains_love             -0.0504      0.146     -0.345      0.730      -0.337       0.236\n",
       "==========================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = df.copy()\n",
    "for word in word_features:\n",
    "        temp['contains_{}'.format(word)] = temp.review_text.str.contains(word)\n",
    "\n",
    "X = temp.iloc[:,5:]\n",
    "X = sm.add_constant(X)\n",
    "y = (temp['rating']=='pos')\n",
    "\n",
    "y = np.array(y, dtype=float)\n",
    "X = pd.DataFrame(np.array(X, dtype=float))\n",
    "X.columns = ['const'] + temp.iloc[:,5:].columns.to_list()\n",
    "\n",
    "lr = Logit(y, X).fit()\n",
    "lr.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Category sentence level cooccurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review_sents'] = df['review_text'].apply(lambda x: nltk.sent_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_sents_with_theme(x, theme):\n",
    "    ct = 0\n",
    "    for sent in x:\n",
    "        if contains_theme(sent, theme):\n",
    "            ct += 1\n",
    "    return ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_sents_with_theme_condition(x, theme_condition_on, theme_to_count):\n",
    "    ct = 0\n",
    "    for sent in x:\n",
    "        if (contains_theme(sent, theme_condition_on)) & (contains_theme(sent, theme_to_count)):\n",
    "            ct += 1\n",
    "    return ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def theme_cooccur(df, text_col, theme_list):\n",
    "    df['sent_col'] = df[text_col].apply(lambda x: nltk.sent_tokenize(x))\n",
    "    df['sent_ct'] = df.sent_col.apply(lambda x: len(x))\n",
    "    for theme in theme_list:\n",
    "        df['{0}_sent_ct'.format(theme)] = df['sent_col'].apply(lambda x: count_sents_with_theme(x, theme))\n",
    "                                                                                                \n",
    "    for theme in theme_list:\n",
    "            print('\\n')\n",
    "            print('Prob of {0}: '.format(theme), df['{0}_sent_ct'.format(theme)].sum() / df['sent_ct'].sum())\n",
    "            for theme_2 in theme_list:\n",
    "                if theme != theme_2:\n",
    "                    df['{0}_{1}_sent_ct'.format(theme, theme_2)] = df['sent_col'].\\\n",
    "                        apply(lambda x: count_sents_with_theme_condition(x, theme, theme_2))\n",
    "                    print('Prob of {0} given {1}:'.format(theme, theme_2),\\\n",
    "                      df['{0}_{1}_sent_ct'.format(theme, theme_2)].sum() / df['{0}_sent_ct'.format(theme_2)].sum())\n",
    "                    \n",
    "                    nobs = [df['{0}_sent_ct'.format(theme_2)].sum(), df['sent_ct'].sum()]\n",
    "                    ct =  [df['{0}_{1}_sent_ct'.format(theme, theme_2)].sum(), df['{0}_sent_ct'.format(theme)].sum()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Prob of ['humor', 'humorous', 'funny', 'comedy', 'laugh', 'joke', 'goofy']:  0.044902980484258795\n",
      "Prob of ['humor', 'humorous', 'funny', 'comedy', 'laugh', 'joke', 'goofy'] given ['bad', 'horrible', 'terrible', 'poor']: 0.0683453237410072\n",
      "\n",
      "\n",
      "Prob of ['bad', 'horrible', 'terrible', 'poor']:  0.027204607728009842\n",
      "Prob of ['bad', 'horrible', 'terrible', 'poor'] given ['humor', 'humorous', 'funny', 'comedy', 'laugh', 'joke', 'goofy']: 0.04140722291407223\n"
     ]
    }
   ],
   "source": [
    "theme_cooccur(df, 'review_text', [comedy_theme, ['bad', 'horrible', 'terrible', 'poor']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mentions of comedy increase mentions of negative directional language and vice versa. Maybe a genre to avoid if you're in the business of making good films."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Prob of ['biography', 'life', 'biopic']:  0.023863445730582116\n",
      "Prob of ['biography', 'life', 'biopic'] given ['good', 'great', 'outstanding', 'excellent']: 0.02159880834160874\n",
      "\n",
      "\n",
      "Prob of ['good', 'great', 'outstanding', 'excellent']:  0.05631046245037186\n",
      "Prob of ['good', 'great', 'outstanding', 'excellent'] given ['biography', 'life', 'biopic']: 0.050966608084358524\n"
     ]
    }
   ],
   "source": [
    "theme_cooccur(df, 'review_text', [['biography', 'life', 'biopic'], ['good', 'great', 'outstanding', 'excellent']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mentions of biopics do not increase usage of positive directional langauage."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
